{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/cookbook/blob/main/gen-ai/agents/video-agent.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/aurelio-labs/cookbook/blob/main/gen-ai/agents/video-agent.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekTh00NOFnOZ"
      },
      "source": [
        "# Chat with Video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJf8rvVIFnl-"
      },
      "source": [
        "In this example we're going to work through building a \"chat-with-video\" AI pipeline and agent. We'll see how to:\n",
        "\n",
        "1. Take any YouTube video and transcribe it to text using Aurelio's video-to-text endpoint.\n",
        "2. Use Mistral LLMs to chat with our transcribed video content.\n",
        "3. Add chat history to make our AI conversational.\n",
        "4. Integrate async and streaming for a better UX and improved scalability.\n",
        "5. See how we can optimize costs by reducing overall token count using semantic similarity, using Aurelio's chunking endpoint and Mistral's embedding models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hc3OuwuNmaRb"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "  aurelio-sdk==0.0.18 \\\n",
        "  \"yt-dlp[default]==2025.2.19\" \\\n",
        "  mistralai==1.5.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsD7-ytckdTr"
      },
      "outputs": [],
      "source": [
        "!yt-dlp https://www.youtube.com/watch?v=JaHfCrVTYF4 -f mp4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "salkxHIostF9"
      },
      "source": [
        "We will use the [Aurelio Platform](https://platform.aurelio.ai/) for both video processing _and_ later for chunking. To follow the tutorial you can use the coupon `JBVIDEOAGENT` for free credits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sleO9IX0myXj"
      },
      "outputs": [],
      "source": [
        "from aurelio_sdk import AurelioClient\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"AURELIO_API_KEY\"] = os.getenv(\"AURELIO_API_KEY\") or \\\n",
        "    getpass(\"Enter your Aurelio API key: \")\n",
        "\n",
        "client = AurelioClient(api_key=os.environ[\"AURELIO_API_KEY\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaVuNZy9msnF"
      },
      "source": [
        "Now we send our video to Aurelio Platform for processing and chunking:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8w7CLXknrD0"
      },
      "outputs": [],
      "source": [
        "response_video_file = client.extract_file(\n",
        "    file_path=\"/content/AI Agents as Neuro-Symbolic Systemsï¼Ÿ [JaHfCrVTYF4].mp4\",\n",
        "    quality=\"low\", chunk=False, wait=-1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erJAXpbVeiRZ"
      },
      "source": [
        "We can access the transcribed video like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "omo2FI_HrdRA",
        "outputId": "6b695399-4fcb-4571-ce2b-1ae363388b15"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" Okay, so I wanted to put together a sort of overview video of what I'm currently working on, which is thinking or restructuring the way that I'm thinking about agents and the way that I'm also teaching or talking about agents. So this isn't going to be like a fully sort of edited and structured video. I just want to show you a little bit of what I'm thinking about and explain or explain where I'm coming from really.  So all in all, this is part of actually a broader thing that I am working on, which is actually why I haven't been posting on YouTube specifically for quite a while. And I think it's almost two months, which is the longest I think I haven't posted in forever. And, you know, it's well, okay, it's because I'm working on this, but it's also for other things as well. I had my first son like a month ago. So I've been pretty busy there. and just working on a lot of things over Aurelio as well. But I wanted to go through this introduction to  AI agents article that I'm working on. And it's done, but I do want to put together a more structured video and some sort of course details on this. There's even a already a code example for this, which is just taking a look at the React agent, which obviously I wanted it earlier. Not earlier, it's probably, I would say, like the foundational structure for what a, agents look like today? Or when I say agents, I mean LM-based agents.  And I think, well, that is just the most popular type of agent. Okay, React, now it's more like tools or tool agents, but they're very similar. Anyway, I'll talk a little bit about them. So the first thing I do want to maybe cover very quickly is the React agent, because that's what we're most familiar with. So I'll come down to here. So as a reminder, okay, React is basically this. here. So we have some input, okay, some types. And rather than just  asking our LLM to answer directly, we allow our LLM to go through multiple reasoning steps. And part of those reasoning steps is the fact that the agent can also call tools. So it can get some external information or do something else. And that's what I'm visualizing here, right? So we have our question. This is from the React paper, which I have linked to. But the example is, okay, aside from the Apple remote, what other device can control the program Apple remote was originally designed to interact with. Probably, to be honest, most ALM can answer this directly now, I think.  particularly given that the this example is from the React paper which is like two years ago but anyway soft topic we're just giving an example here right so in this example what we're doing is okay we have these tools that the LM can use we provide them and we're also prompting the agent the LM sorry to say okay go through these multiple steps of reasoning and action so that is where the React comes from so it's RE and act from action. And it goes through these steps. So it's like, okay, I have access to the search tool.  And like an answer tool at the bottom here, which not really at all, but it kind of is at the same time. So it goes through, has this prompt, and it knows I have to structure things in this React methodology. So that's what it does. It says, okay, it starts. It says, okay, I need to search Apple remote and find a program is used for, and then it provides its structures an action based on that. So it knows it has a search tool and it knows that the input to the search tool is a query, which is a string. So we have the app or remote. That function runs using some logic we've developed.  And the observation from that is this. The Apple Remote is designed to control the front row media center. Okay. So what was the question? We have aside from the Apple Remote what other device can control the program Apple Remote was originally decided to interact with. So now we know what that original program was, right, which is front row. And we have that information. The LM now knows that information. So it goes on to a next second. It says, okay, what do I need to do now? I know that Apple remote controls the front row program.  but what other device controls the front row program. So it says, okay, based on this, my next reasoning step is I need to search front row and find other devices that control it. So then it does this search for front row. It could also probably do something, if we're thinking in rag terms here, it could be like device to control front row. And probably more today LM would do that. But that's fine. this is just an example. So it goes back to the search tool again and it says query. Front row. And this isn't  I've shortened this for the sake of brevity. I think in the actual example, or at least from the paper, the actual example returns a lot more information. But this is the part of it that is important. The front row is controlled by an Apple remote or keyboard function keys. Okay, so now we know that. That gets fed back into the LM. So the LM now knows everything that we've covered here, knows the original query. And now it's like, okay, well, I have all the information that I need to answer the original query which is site to a map or remote what other device can control front row  So the next step is the LM is like, oh, okay, I have everything. I can now provide the answer of keyboard function keys to the user. Okay. And so it doesn't use a search tool. Now it instead uses the answer tool, which has this query, this, sorry, parameter out. And the answer or the output for that is keyboard function keys, which then gets provided back to the user. that. Okay. So this is the React agent and this sort of structure of like reasoning, building  a like a query for a tool, getting a response, and then potentially going through another iteration of reasoning and action and another and then eventually providing an answer. That is really the sort of commonly accept a definition of what an agent is. That's what most people are using at the moment. And I think that is great, but I think it's very limiting. because I just wouldn't, in production, I would never just put something like this, whether it's React or Open AI tools or whatever else.  I wouldn't just put that. In my opinion, an agent is much broader than just what this is. And also in general, broader literature, an agent is not just this either. So I went back and I just went through a few papers trying to figure out, okay, what is the actual good definition of an agent that kind of makes sense in the way that I also understand the agents, the way that I've been building like agentic, more workflows to be honest, right? But to me, workflow or agent is kind of  Same thing. It's agentic workflow, i.e. agent. So, anyway, I went back and the paper that I think had the nicest definition that tied back to really like original, like, AI, well, philosophy, like the original AI philosophy or the original AI research, maybe not original, but pretty close to original, and I don't think maybe original, was this, right? So it was a miracle paper, right, which is another...  basically agent, LM agent. I think this came just before the React agent paper. It's very similar, I would say, has a bit less structured in the React agent. But, yeah, it's super relevant. And the way that they described their system was that it was a neurosymbolic architecture. I really like this definition because a, so neurosymbolic architecture, It's two things, right? You have the neural part, you have the symbolic part. And I actually have another kind of starting  on this article but it's uh yeah there's this mostly notes at the moment so the neural part of this in fact let's start with the symbolic part the symbolic part is the more traditional AI right so the you know I think this is back in the 40s 50s 60s mostly and then maybe so actually 70s as well this was actually maybe not 70s this was this was a the sort of traditional approach to AI. And the idea,  or the symbolists that were just like full on symbolists felt that true AGI would be achieved through written rules, ontologies and these other logical functions to basically a load of handwritten stuff, like smart, like philosophical grammars. An example of this is the, I think it's syllogistic logic from Aristotle. and the, so basically, an example of this would be a, I think it's got,  you have this major premise then you have a minor premise and i haven't done this for a long time so forgive me if i'm not super accurate um but you have a major premise minor premise and conclusion based on that so the idea is like if you say something like um all all dogs have four legs which is maybe not actually true but let's just assume that you like all dogs have four legs by nature. Okay.  So let's just remove that bit. Let's not be too pedantic. All dogs have four legs, right? That is your major premise. Then you would say, um, my friend, Japs is a dog. Okay? Your conclusion would be, okay, um, my friend Japs has, has four laps. Okay, so this is a logical framework developed by Arisothel.  And the symbolic AI people would, you know, do things like these, these sort of exercises where they're going through all this and trying to build up some sort of logical methodology to allow you to kind of construct some deeper, like aGI type system where it can just kind of figure everything out. Now, that was like one side of AI back then. And this is like the traditional AI, it's also called like good,  old fashioned AI. I don't remember who or when that was turned, but gofi. I don't know if they actually call it gofi, but that's how it's written. And yeah, I mean, that was one camp. The other camp were the connectionists. So this is what we, we call them back then. Now it's kind of the neural AI type thing. So connectionism was in, so kind of emerged back in 9. there was this basically a paper that described a neural circuit.  But really, the where neural or connectionist AI really started with is with this guy, Rosenblatt, who introduced this idea of a perceptron. And it's actually the perceptron is in an adapted version of the perceptron that he described is what we use in neural networks today. So it was, okay, now it's a big deal. But back then, they were less useful, but a lot of people really believed in it. And they were probably,  at least so far, they were more correct, I would say. Now, the connectionist's approach is focused on building AI systems loosely based on the mechanisms of our brains. So neural network, perceptron is just like a kind of silly name. Now we would say things like neurons within the neural network. They all have these sort of names, right? Whereas you can tell they're kind of coming from the idea of a brain. I don't have an example here, but you can see, okay,  have a look at in Google. This is the perception, right? And then if you look at a sort of a neuron diagram, if that's a thing, activation, you know, kind of, there's something, right? So here you have a, like an actual neuron diagram. And you can see there's a lot of similarity. I think this one, they're probably comparing it to an actual neuron in the sort of AI sense. Actually, here is  is a perfect example. Let me make this bigger. No. Right. So this is a good example. On the left, you have all these inputs, basically for your neuron in your brain. Goes through some kind of calculation, which in this case is the axon. And then you have all these outputs. And this is actually many outputs, but you can think of them as kind of similar in some way. Because all of these axons here, to be fair, I think they have different degrees. degrees of activation. But when you get your  output here, you just have sort of one output. So I suppose in some degree this would be different. But just when you look at a single neural network neuron, obviously we put many of these in many layers and then at that point you have many sort of axons, although each one is just coming from a single output here. But any case, there is definitely a lot of similarity here. So yeah, anyway, that is kind of, you know, one fundamental.  building blocks of neural AI. And yeah, for neural AI to work in a lot of compute, parallel processing, all this sort of stuff. And because of that, it didn't really kick off. And there was a few like AI, what we call AI winters, where people were just less interested in AI in general, but particularly the neural or connectionist AI. And yeah, I mean, that kind of carried on into, into the future until we got towards like 2011, 2012 where you had image net and  the, what is it called, the AlexNet model. And they sort of kicked off interest in neural or connectionist AI again. And at that point, it's just like neural networks. Everyone's like, wow, neural networks are amazing. And we still think that. That's what transformers and LLMs and their core building block is, well, they are a type of neural network. Just more kind of big and complicated. Anyway, so that kicked off because we had loads of data and compute and everything and  And yeah, it led to where we are now, right? So that's what the neural part is, and that's also what the symbolic part is here. Right? So, okay, what do we have here? We have both. We're mixing the old traditional AI with neural AI. Well, kind of. To some degree, they are almost kind of mixed together already with neural networks, because neural networks, the way that they work, they almost learn symbols, like they learn logical representations of different concepts, which is what the symbol,  part is in some symbolic they learn these right but they're just not handwritten okay so you know neural network kind of learns what are what a strawberry is or what a dog is but anyway it's kind of side the point um we can just assume okay maybe maybe neural networks are subs symbolic but for now let's just assume they're purely symbolic that's fine so neural networks make up the neural part of this so basically LLMs, then we have this symbolic part. The symbolic part, as I mentioned before, it's handwritten stuff, right? So...  like code. So if you write some like some code that can be run by a or triggered by an LLM or some other type of neural network that you have some sort of neural's neuro symbolic architecture. You have a mix of both. So that's that is what they that is right and when they develop the miracle system here they're using I think it's like GPD to no maybe but jubtie 3 um but like the first version of it which was not that great and then they were  with so this was actually their sort of agent system but I think they built at least part of this on top of an I'm not sure if it was open source model I don't remember the name of it to be honest for the life of me but anyway it doesn't matter so so they basically built this agentic type thing by mixing neural networks with runable code yeah and then and then you actually see some of the things that they're talking about here are you know kind of and the things that we try and solve with rag in many cases, lack of up-to-date knowledge, like proprietary knowledge,  all these sort of things, which is kind of interesting, I think. But anyway, so my definition of agents kind of goes along those lines. It's neural plus symbolic. And the reason I like it is one, we have that sort of, that definition is anchored in, you know, the AI for the past almost 100 years, maybe 8. 80 years roughly, which is great. I think it's good that we have some like really, very, very  solid foundations behind that definition, neuro-symbolic. And two, one of the reasons I like it is because when I'm building these systems, okay, LMs are great, but I don't just use LLMs. A lot of time, there is very good reason to bring in other neural network-based models. So by broadening that neural definition to neural network, you don't restrict yourself to just saying LLM, right?  hey, use our limbs, like amazing. And of course, I use them a lot, but not just our lamps, right? So the idea behind, if I go to semantic router, I don't know if you've used it, not a big deal if you haven't. But the idea behind semantic router, let me find an image instead. Actually, maybe I have an introduction here. Wow, we don't have an introduction. Okay. Okay, so this is a better  example or easy to explain an example. So semantic router uses embedding models, which are on neural network based. And what they do is you have some text. I have a better image somewhere. Let me find it. Okay, so this is the other example. So we have an embedding model, the thing in the middle here. And what we do is we provide some example inputs. So it's like political route, just this is more for like a guard rail right so this would be a this would be an actual guard rail here as you know  like protection basically and okay that's fine whatever that that is just one example then we have the ask lm route and this is a better example so s lm route all right so i'm saying okay what is the lama 2 model llama 3 now i wrote this a long time ago tell me about metas new lm what are the differences between falcon and llama right all of those are obviously things i want to trigger a search right so what I can do is I can I can identify this with the embedding model so I can say okay anything that gets caught in this little area here  anything caught in that area there, that is probably the user asking for us to do a search, essentially. So then what we can do, what is many things we can do, but one thing we can just do is say, okay, that's the user query, just send it straight across to some rag pipeline, right? So don't even ask the LLM, don't even ask an LM to rephrase it or, you know, make a decision to use the RAD pipeline, just use the Rite pipeline directly. and it's way faster than going through an LM and I would say probably much more controllable. However,  LMs provide a lot of flexibility. So that's not what I would usually do. Instead, what I usually do is there's still an LM, right? So let's say over here, right? I have an LM. And what I do is, okay, we have our query that we got from the user. What I'm gonna do is I'm just gonna, I'm gonna modify it a little bit. All right, so I'm gonna come over here. And this is just like, kind of lazy way, It works well and it leaves the flexibility of use down to the LLM, which I like.  So I say, okay, original query right from the user. So whatever that was, so we have the query. And then I append something extra. So I say, like system note is something I've used fairly often before. And I'll just say use the rag tool. Okay. I don't like this new. Maybe that is a nice fun. It's fine. Whatever, I'm using this one now. Right. So I modified it. query that gets sent to the LM. And in the LM, you're basically kind of heavily suggesting to the LM  what it should do and that works actually very well. So this sort of system, you know, the agent is not just the LM. It's also the embedding here, right? And especially if you're not even, you know, not even including an LM here, right? There's like to me, this system without an LM is pretty agentic to me, right? that seems to me to be an agent. And then even more so when you add the LM and decision making in there. So yeah, I, I,  I prefer to think of agents as this type of system, right? Or not just this type of system, but at least more flexibly. Because I think that if you think of agents just as an LLM that can call tools, you're massively limiting, you know, you're boxing yourself into this one thing that an agent may be. Whereas I think that's kind of a stupid thing to do. and even if you take the example of, okay, let's say we have multiple like tool sets or we have, okay, once we've this  decided on one tool. Okay, let's start here. Right. Let's say we make a decision. Now, LM is making this decision. That's fine. No problem. Right, but it goes down these two different paths. All right. So it says tool A or tool B. And let's say if we have used tool A for, you know, whatever that is, maybe it's reading about the news. Whereas tool B is, oh, it's someone's asking a math question so you know that it's like a calculator or it's actually maybe it's searching for some explanation from like a like a math website right where is this  going to a news website, right, two different use cases. And what you might find with these two different use cases is that the follow-on tools, if any, right, maybe there aren't any, who knows, right, but maybe there are. And the following tools for these would be different, right? I mean, you've already identified that the intent is two very different things. So why would the follow-on tools be similar? There's no reason for them to be. So in this case, right? So you still have an alum in the middle, maybe, maybe sometimes you own, but  you would then, you know, follow this slightly different path, right? And if you're, if you're thinking of agents as just, oh, it's a LM plus some tool calls in a loop, right? You already, this is fairly simple and you can't do it. So my, yeah, that's what I'm thinking about with agents. That's how I would approach them, which is slightly different to, I think, what, like what the, the standard sort of narrative is from most people on what an agent is, right?  So, okay, it's valid, but it's not all that an agent is. So I'm going to leave that. I don't want to go, you know, there's a ton of stuff I can talk through. I'll restrict it to this one thing for now. I will cover this with more structure fairly soon. And hopefully we'll ramble a little bit less. But at least I think with this, you should get an idea of where I'm coming from. And the hopefully, sensible to some degree logic behind what I'm thinking here. But anyway.  that's it so thank you for watching I will definitely try and make sure to release something else very soon but for now I'll leave it there thank you very much watching I will see you in the next one bye You know,\""
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "content = response_video_file.document.content\n",
        "content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jhWMXN1fPsF"
      },
      "source": [
        "We can count the number of words from our transcribed video:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7P2_Ppvrt9M",
        "outputId": "8557bdf3-6d38-4bfe-b8ce-5a18d7b33c4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4152"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(content.split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxNEVO9BUsxJ"
      },
      "source": [
        "## Connecting an LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B5n-85JWbkd"
      },
      "source": [
        "We're using Mistral AI in this example and we'll be using both their LLM and embed models, you can get [an API key from here](https://console.mistral.ai/api-keys)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Q7rTQEmLYCR3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from mistralai import Mistral\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"MISTRAL_API_KEY\"] = os.getenv(\"MISTRAL_API_KEY\") or \\\n",
        "    getpass(\"Enter your Mistral API key: \")\n",
        "\n",
        "client = Mistral(api_key=os.environ[\"MISTRAL_API_KEY\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vIKVQeCnb01r"
      },
      "outputs": [],
      "source": [
        "from mistralai.models import (\n",
        "    SystemMessage,\n",
        "    UserMessage\n",
        ")\n",
        "\n",
        "response = client.chat.complete(\n",
        "    model=\"mistral-large-latest\",\n",
        "    messages=[\n",
        "        SystemMessage(content=(\n",
        "            \"You are an AI expert providing help to the user \"\n",
        "            \"based on the content of the provided transcribed \"\n",
        "            \"document.\\n\\n---\\n\\nTranscription:\\n\\n\" +\n",
        "            content\n",
        "        )),\n",
        "        UserMessage(content=\"Hi can you summarize this for me?\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwSZvInACmuS"
      },
      "source": [
        "We get our message content like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_R0bp_kP6VR",
        "outputId": "bb0b7ee5-72b6-486c-dda0-d1a297a1ae13"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AssistantMessage(content=\"Sure! Here's a summary of the transcribed document:\\n\\n### Overview:\\nThe speaker is working on restructuring their approach to AI agents and plans to create a more structured video and course on the topic. They have been busy with personal life events and other projects, which has delayed their YouTube posting.\\n\\n### Key Points:\\n1. **Introduction to AI Agents**:\\n   - The speaker discusses the React agent, a foundational structure for LLM-based agents.\\n   - React agents use multiple reasoning steps and can call external tools to gather information or perform actions.\\n\\n2. **Example of a React Agent**:\\n   - An example is provided where an agent answers a question about the Apple Remote by using a search tool to gather information in multiple steps.\\n   - The agent uses a search tool to find out which program the Apple Remote controls and then searches for other devices that can control that program.\\n\\n3. **Definition of Agents**:\\n   - The speaker argues that the common definition of agents (LLM plus tool calls) is limiting.\\n   - They introduce the concept of a neuro-symbolic architecture, which combines neural networks (LLMs) with symbolic AI (handwritten rules and code).\\n\\n4. **Historical Context**:\\n   - Symbolic AI involves logical reasoning and handwritten rules, often associated with traditional AI from the mid-20th century.\\n   - Neural AI, or connectionism, is based on neural networks and perceptrons, which mimic brain mechanisms and emerged in the 1990s.\\n\\n5. **Neuro-Symbolic Architecture**:\\n   - This architecture combines the strengths of both neural and symbolic AI.\\n   - The speaker uses embedding models and other neural network-based models in their systems, not just LLMs.\\n\\n6. **Example of a Neuro-Symbolic System**:\\n   - The speaker describes a semantic router that uses embedding models to route queries to different tools or pipelines without always involving an LLM.\\n   - This approach allows for more flexibility and control in the agent's decision-making process.\\n\\n### Conclusion:\\nThe speaker believes that agents should be thought of more broadly than just LLMs calling tools. They advocate for a neuro-symbolic approach that incorporates various neural network-based models and symbolic AI components. This broader definition allows for more flexible and effective agent systems.\\n\\n### Next Steps:\\nThe speaker plans to create a more structured video and course on this topic soon.\", tool_calls=None, prefix=False, role='assistant')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.choices[0].message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSuB_nf2QDiP"
      },
      "source": [
        "We can also track our usage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPO_BvjfQGO3",
        "outputId": "bdea6a01-4aeb-46f2-8e02-5ce5505474dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "UsageInfo(prompt_tokens=5430, completion_tokens=546, total_tokens=5976)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUmAVPhUCs88"
      },
      "source": [
        "## Adding Chat History"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBWZxTZ7Cvs6"
      },
      "source": [
        "To make our video chat conversational we need to maintain chat history â€” to do that we'll write an `Agent` class that we can initialize and interact with, it will maintain our messages within this class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qyC6UWSuoi0A"
      },
      "outputs": [],
      "source": [
        "from mistralai.models import AssistantMessage, UsageInfo\n",
        "\n",
        "class Agent:\n",
        "    messages: list[AssistantMessage | SystemMessage | UserMessage]\n",
        "    usage: list[UsageInfo]\n",
        "\n",
        "    def __init__(self):\n",
        "        self.messages = [\n",
        "            SystemMessage(content=(\n",
        "                \"You are an AI expert providing help to the user \"\n",
        "                \"based on the content of the provided transcribed \"\n",
        "                \"document.\\n\\n---\\n\\nTranscription:\\n\\n\" +\n",
        "                content\n",
        "            ))\n",
        "        ]\n",
        "        self.usage = []\n",
        "\n",
        "    def chat(self, content: str) -> AssistantMessage:\n",
        "        # append user message to self.messages\n",
        "        self.messages.append(UserMessage(content=content))\n",
        "        # generate response\n",
        "        response = client.chat.complete(\n",
        "            model=\"mistral-large-latest\",\n",
        "            messages=self.messages\n",
        "        )\n",
        "        # append assistant message to self.messages\n",
        "        self.messages.append(response.choices[0].message)\n",
        "        # append usage (we can use this later)\n",
        "        self.usage.append(response.usage)\n",
        "        return response.choices[0].message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc-iJPlV3Hl7"
      },
      "source": [
        "Now we can chat with our conversational history agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "QJMRF_U13Nuy",
        "outputId": "450022d9-d1cf-48ba-a7f0-277182035505"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "In the context of the article, 'symbolic' refers to a traditional approach to artificial intelligence that involves using handwritten rules, ontologies, and logical functions to create AI systems. This method, often called \"Good Old-Fashioned AI\" (GOFAI), relies on explicit, predefined knowledge structures to enable reasoning and decision-making. The article contrasts this with the 'neural' approach, which uses neural networks and machine learning to learn patterns and representations from data. The combination of these two approachesâ€”symbolic and neuralâ€”is referred to as a neurosymbolic architecture, which the article suggests is a more comprehensive and flexible way to define and build AI agents."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "# initialize\n",
        "agent = Agent()\n",
        "\n",
        "res = agent.chat(\n",
        "    content=\"can you summarize the meaning of 'symbolic' in this article?\"\n",
        ")\n",
        "# print output in markdown\n",
        "display(Markdown(res.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB9GA5545ICd"
      },
      "source": [
        "Now let's ask another question which requires context of the previous interactions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "ErB0t7a_5L49",
        "outputId": "e5564c4f-b754-48a9-d174-27ff611aaa69"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Certainly! Here are the key points about 'symbolic' in the article:\n",
              "\n",
              "- **Traditional AI Approach**: Involves using handwritten rules, ontologies, and logical functions.\n",
              "- **Good Old-Fashioned AI (GOFAI)**: Relies on explicit, predefined knowledge structures for reasoning and decision-making.\n",
              "- **Contrast with Neural AI**: Unlike neural networks, which learn from data, symbolic AI uses predefined logic.\n",
              "- **Neurosymbolic Architecture**: Combines symbolic AI (handwritten rules) with neural AI (neural networks) for a more comprehensive and flexible AI system.\n",
              "- **Agent Definition**: The article suggests that agents should be defined as neurosymbolic systems, integrating both symbolic and neural components.\n",
              "\n",
              "This summary captures the essence of how 'symbolic' is discussed in the article."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "res = agent.chat(\n",
        "    content=\"can you give me that but in short bullet-points?\"\n",
        ")\n",
        "# print output in markdown\n",
        "display(Markdown(res.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkT9G5tCLCZR"
      },
      "source": [
        "## Async and Streaming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObxCjinJTOfZ"
      },
      "source": [
        "When developing AI apps that rely heavily on external APIs we tend to write async code to make our applications more scalable. With async code the time that our code would be spent waiting for API responses can instead be spent performing other tasks.\n",
        "\n",
        "We will rewrite our `Agent` class to work fully asynchronously, and we'll also add streaming â€” which can provide an improved user experience as we can show the user the tokens as soon as they're generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "DKBiCzqnS75z"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    messages: list[AssistantMessage | SystemMessage | UserMessage]\n",
        "    usage: list[UsageInfo]\n",
        "\n",
        "    def __init__(self):\n",
        "        self.messages = [\n",
        "            SystemMessage(content=(\n",
        "                \"You are an AI expert providing help to the user \"\n",
        "                \"based on the content of the provided transcribed \"\n",
        "                \"document.\\n\\n---\\n\\nTranscription:\\n\\n\" +\n",
        "                content\n",
        "            ))\n",
        "        ]\n",
        "        self.usage = []\n",
        "\n",
        "    async def chat(self, content: str) -> AssistantMessage:\n",
        "        # append user message to self.messages\n",
        "        self.messages.append(UserMessage(content=content))\n",
        "        # generate response asynchronously\n",
        "        response = await client.chat.stream_async(\n",
        "            model=\"mistral-large-latest\",\n",
        "            messages=self.messages\n",
        "        )\n",
        "        # full response object to be built\n",
        "        all_tokens = []\n",
        "        all_usage = []\n",
        "        # iterate through the token generator and add to queue\n",
        "        async for chunk in response:\n",
        "            if (token := chunk.data.choices[0].delta.content) is not None:\n",
        "                print(token, end=\"\", flush=True)\n",
        "                all_tokens.append(token)\n",
        "        # append assistant message to self.messages\n",
        "        self.messages.append(AssistantMessage(content=\"\".join(all_tokens)))\n",
        "        # append usage (we can use this later)\n",
        "        self.usage.append(chunk.data.usage)\n",
        "        return self.messages[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pm4TJWNI8RhY",
        "outputId": "74288c54-11fa-4d5e-9b34-2fead40b3b22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In the context of the transcribed document, \"symbolic\" refers to a traditional approach in artificial intelligence (AI) that involves handwritten rules, ontologies, and logical functions to build AI systems. This approach is often contrasted with \"neural\" or connectionist AI, which relies on neural networks and machine learning.\n",
            "\n",
            "Key points about the symbolic approach mentioned in the document:\n",
            "\n",
            "1. **Historical Context**: The symbolic approach was prominent in the mid-20th century, often referred to as \"good old-fashioned AI\" (GOFAI).\n",
            "2. **Logical Frameworks**: It involves logical methodologies, such as syllogistic logic from Aristotle, where conclusions are derived from premises.\n",
            "3. **Handwritten Rules**: Symbolic AI systems are built using manually crafted rules and ontologies, aiming to create systems that can reason logically.\n",
            "4. **Comparison with Neural AI**: Unlike neural AI, which learns from data and mimics the structure of the brain, symbolic AI relies on predefined logical structures.\n",
            "\n",
            "In the context of neuro-symbolic architecture, the symbolic part refers to the handwritten, logical components that can be integrated with neural networks to create more robust and flexible AI agents."
          ]
        }
      ],
      "source": [
        "agent = Agent()\n",
        "\n",
        "res = await agent.chat(\n",
        "    content=\"can you summarize the meaning of 'symbolic' in this article?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA5w6SF6Dle8"
      },
      "source": [
        "We can continue our conversation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKGTZCiwDoYm",
        "outputId": "ef2481b6-d4e9-4c26-8a81-fffb1ebe1273"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In the transcribed document, point (1) discusses the historical context of the symbolic approach in artificial intelligence (AI). Here's a more detailed breakdown:\n",
            "\n",
            "**Historical Context**:\n",
            "\n",
            "* The symbolic approach was one of the earliest methods used in AI, emerging in the mid-20th century.\n",
            "* It was prominent during the 1940s, 1950s, and 1960s, and possibly extended into the 1970s.\n",
            "* This period is often referred to as the era of \"good old-fashioned AI\" (GOFAI), a term coined to distinguish it from more modern approaches like connectionist or neural AI.\n",
            "* During this time, AI research was largely focused on creating intelligent systems through manual engineering, using rules, logic, and symbol manipulation.\n",
            "* The key idea was that human intelligence could be reduced to symbol manipulation, and thus, AI could be achieved by programming machines to manipulate symbols in a similar way.\n",
            "* This approach was heavily influenced by the work of philosophers and mathematicians, such as Aristotle, who developed logical frameworks for reasoning.\n",
            "* The symbolic approach dominated AI research until the AI winters of the 1970s and 1980s, during which funding and interest in AI declined.\n",
            "* The resurgence of interest in neural networks and machine learning in the late 2000s and early 2010s shifted the focus of AI research away from purely symbolic methods.\n",
            "\n",
            "Some key figures and developments during this period include:\n",
            "\n",
            "* **Alan Turing**: His work on computability and the Turing machine laid the groundwork for symbolic AI.\n",
            "* **John McCarthy**: He invented the Lisp programming language, which was designed for symbolic processing and became a major tool in AI research.\n",
            "* **Marvin Minsky**: He was a leading figure in AI and co-founded the MIT Artificial Intelligence Laboratory.\n",
            "* **Herbert A. Simon and Allen Newell**: They developed some of the earliest AI programs, such as the Logic Theory Machine and the General Problem Solver, which used symbolic reasoning.\n",
            "\n",
            "In the context of the transcribed document, the speaker highlights the historical significance of the symbolic approach and its influence on the development of AI as a field. However, the speaker also notes that the symbolic approach has limitations and that a neuro-symbolic architecture, which combines neural and symbolic methods, may be more effective for building AI agents."
          ]
        }
      ],
      "source": [
        "res = await agent.chat(\n",
        "    content=\"tell me in more detail what was said on point (1)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFo0FbCjDzLm",
        "outputId": "270ec2f7-15ac-460a-e6c2-fd18871dbc42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The historical context of the symbolic approach in AI is relevant to AI agents in several ways, as discussed in the transcribed document. Here's how it connects:\n",
            "\n",
            "1. **Foundation of AI Agents**: The symbolic approach provides a foundational understanding of how AI agents can be designed to reason and make decisions. Early AI agents were often rule-based systems that used symbolic reasoning to interpret their environment and make decisions.\n",
            "2. **Neuro-Symbolic Architecture**: The speaker in the document argues for a neuro-symbolic architecture for AI agents, which combines neural ( connectionist) and symbolic methods. The historical context of symbolic AI is crucial for understanding one half of this architecture.\n",
            "3. **Logical Reasoning**: Symbolic AI's focus on logical reasoning and manipulation of symbols is still relevant for AI agents today. Even modern AI agents, which often rely heavily on neural networks, can benefit from incorporating symbolic reasoning to improve their decision-making processes.\n",
            "4. **Handwritten Rules and Ontologies**: In the context of AI agents, handwritten rules and ontologies can be used to:\n",
            "\t* Provide common-sense knowledge and domain-specific knowledge.\n",
            "\t* Define the agent's goals, actions, and constraints.\n",
            "\t* Create interpretable and explainable AI agents.\n",
            "\t* Improve the agent's ability to plan and reason about complex tasks.\n",
            "5. **Integration with Neural Networks**: By integrating symbolic components with neural networks, AI agents can leverage the strengths of both approaches. For example:\n",
            "\t* Neural networks can be used to learn complex patterns and representations from data.\n",
            "\t* Symbolic components can be used to reason about those representations, make decisions, and explain the agent's behavior.\n",
            "6. **Example: Semantic Router**: The speaker mentions an example of a semantic router, which uses an embedding model (neural) to route queries to different tools or routes (symbolic). This illustrates how a neuro-symbolic architecture can be used to create more flexible and efficient AI agents.\n",
            "\n",
            "In summary, the historical context of the symbolic approach in AI is directly relevant to AI agents, as it provides a foundation for understanding and designing agents that can reason, make decisions, and interact with their environment. Moreover, integrating symbolic components with neural networks in a neuro-symbolic architecture can lead to more robust, flexible, and efficient AI agents."
          ]
        }
      ],
      "source": [
        "res = await agent.chat(\n",
        "    content=\"and what does that have to do with AI agents?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL-RRGxTC2_K"
      },
      "source": [
        "## Optimizing Token Cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlC3a0l-C7GB"
      },
      "source": [
        "Throwing the full article into each interaction will give us maximal accuracy but is also _expensive_.\n",
        "\n",
        "We can see this by checking our token usage and calculating our costs. To check our usage we simply access our agent's `usage` attribute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akf2T_cGEDEj",
        "outputId": "d9c3961a-78d7-4f93-e25b-096f2bd51d3c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[UsageInfo(prompt_tokens=5436, completion_tokens=272, total_tokens=5708),\n",
              " UsageInfo(prompt_tokens=5724, completion_tokens=551, total_tokens=6275),\n",
              " UsageInfo(prompt_tokens=6289, completion_tokens=521, total_tokens=6810)]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent.usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgWXWmyTEKF6"
      },
      "source": [
        "To estimate our costs for running our agent, we can take the latest pricing for Mistral's large model for `prompt_tokens` (input) and `completion_tokens` (output) from their [pricing page](https://mistral.ai/en/products/la-plateforme#pricing).\n",
        "\n",
        "As of 08 March 2025 those prices are:\n",
        "\n",
        "| Model | API name | Input (/M tokens) | Output (/M tokens) |\n",
        "| ----- | -------- | ----------------- | ------------------ |\n",
        "| Mistral Large 24.11 | mistral-large-latest | \\$2 | \\$6 |\n",
        "\n",
        "We can define a cost calculator like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "5P3zFl9iEX-f"
      },
      "outputs": [],
      "source": [
        "def cost(usage: UsageInfo) -> float:\n",
        "    input_cost = usage.prompt_tokens * 2e-6\n",
        "    output_cost = usage.completion_tokens * 6e-6\n",
        "    return round(input_cost + output_cost, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAl-rNANGg7O",
        "outputId": "a289101c-7127-454e-c3f8-11556d6667df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "$0.0125\n",
            "$0.01475\n",
            "$0.0157\n"
          ]
        }
      ],
      "source": [
        "# save these values for later reference\n",
        "original_cost = []\n",
        "\n",
        "for usage in agent.usage:\n",
        "    usage_cost = cost(usage=usage)\n",
        "    original_cost.append(usage_cost)\n",
        "    print(f\"${usage_cost}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lrv04LllHXZG"
      },
      "source": [
        "These seem like small numbers but they will quickly add up as we continue throwing the full transcription into our LLM with every new interaction.\n",
        "\n",
        "To optimize our token cost we can pull in _only_ the most relevant chunks of information, making use of semantic similarity. To do this we will:\n",
        "\n",
        "1. Break our transcribed document into smaller chunks.\n",
        "2. Embed those chunks into vector embeddings.\n",
        "3. Store those vector embeddings in a numpy array.\n",
        "4. When querying, our LLM (now agent) will transform our question into a small query.\n",
        "5. We embed that query into a vector embedding.\n",
        "6. Compare the semantic similarity between our query vector and our chunk vectors to find the most similar chunks.\n",
        "7. Return those chunks to our LLM ready for a final response.\n",
        "\n",
        "Let's start by chunking our document, we use the _async_ Aurelio chunking endpoint for this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "D5kmU3LHC6rx"
      },
      "outputs": [],
      "source": [
        "from aurelio_sdk import AsyncAurelioClient\n",
        "\n",
        "# we reinitialize the client for async\n",
        "aurelio_client = AsyncAurelioClient()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p_09ISXMB_Q"
      },
      "source": [
        "Call the chunk endpoint on our document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Y6MXwoY6MBYs"
      },
      "outputs": [],
      "source": [
        "from aurelio_sdk import ChunkingOptions\n",
        "\n",
        "# we use a semantic chunker with a max chunk length of 500 tokens\n",
        "chunking_options = ChunkingOptions(\n",
        "    chunker_type=\"semantic\",\n",
        "    max_chunk_length=500,\n",
        "    window_size=5\n",
        ")\n",
        "\n",
        "chunks = await aurelio_client.chunk(\n",
        "    content=content,\n",
        "    processing_options=chunking_options\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk3N7dwTNX5-"
      },
      "source": [
        "We can see the chunks like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2x80kGzKMX__",
        "outputId": "7d512d43-2175-437e-a46c-6861983cf665"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[ResponseChunk(id='chunk_3447ec47-f78c-427d-ac65-89e5fdc607ea', content=\"I know that Apple remote controls the front row program.  but what other device controls the front row program. So it says, okay, based on this, my next reasoning step is I need to search front row and find other devices that control it. So then it does this search for front row. It could also probably do something, if we're thinking in rag terms here, it could be like device to control front row. And probably more today LM would do that. But that's fine. this is just an example. So it goes back to the search tool again and it says query.\", chunk_index=6, num_tokens=121, metadata={}),\n",
              " ResponseChunk(id='chunk_935696c3-3bfa-485b-a305-8e94166f3eb6', content=\"Front row. And this isn't  I've shortened this for the sake of brevity. I think in the actual example, or at least from the paper, the actual example returns a lot more information. But this is the part of it that is important.\", chunk_index=7, num_tokens=54, metadata={}),\n",
              " ResponseChunk(id='chunk_ec626319-c0fb-4658-b94d-3013f95b2c62', content=\"The front row is controlled by an Apple remote or keyboard function keys. Okay, so now we know that. That gets fed back into the LM. So the LM now knows everything that we've covered here, knows the original query. And now it's like, okay, well, I have all the information that I need to answer the original query which is site to a map or remote what other device can control front row  So the next step is the LM is like, oh, okay, I have everything.\", chunk_index=8, num_tokens=104, metadata={}),\n",
              " ResponseChunk(id='chunk_78c7131c-9ce5-4e6c-bba2-76df718b7a50', content=\"I can now provide the answer of keyboard function keys to the user. Okay. And so it doesn't use a search tool. Now it instead uses the answer tool, which has this query, this, sorry, parameter out. And the answer or the output for that is keyboard function keys, which then gets provided back to the user. that.\", chunk_index=9, num_tokens=70, metadata={}),\n",
              " ResponseChunk(id='chunk_8abce96a-3adb-4a58-bf04-571ac886fb75', content=\"Okay. So this is the React agent and this sort of structure of like reasoning, building  a like a query for a tool, getting a response, and then potentially going through another iteration of reasoning and action and another and then eventually providing an answer. That is really the sort of commonly accept a definition of what an agent is. That's what most people are using at the moment. And I think that is great, but I think it's very limiting. because I just wouldn't, in production, I would never just put something like this, whether it's React or Open AI tools or whatever else. I wouldn't just put that. In my opinion, an agent is much broader than just what this is. And also in general, broader literature, an agent is not just this either. So I went back and I just went through a few papers trying to figure out, okay, what is the actual good definition of an agent that kind of makes sense in the way that I also understand the agents, the way that I've been building like agentic, more workflows to be honest, right? But to me, workflow or agent is kind of  Same thing. It's agentic workflow, i.e. agent. So, anyway, I went back and the paper that I think had the nicest definition that tied back to really like original, like, AI, well, philosophy, like the original AI philosophy or the original AI research, maybe not original, but pretty close to original, and I don't think maybe original, was this, right? So it was a miracle paper, right, which is another.\", chunk_index=10, num_tokens=329, metadata={})]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks.document.chunks[5:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpMsluS-N1io"
      },
      "source": [
        "Next, we need to embed each chunk. For that we will use Mistral's embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "J0g6x1SuNK4X"
      },
      "outputs": [],
      "source": [
        "embeddings_response = await client.embeddings.create_async(\n",
        "    model=\"mistral-embed\",\n",
        "    inputs=[x.content for x in chunks.document.chunks]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtW8CP5-PvFE"
      },
      "source": [
        "This returns a list of `EmbeddingResponseData` objects inside the `.data` attribute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uar1GBhrO8J9",
        "outputId": "6bbd8cfc-8952-4255-8d10-df93e10e380d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(embeddings_response.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofEcEaqSP3Ov"
      },
      "source": [
        "Each of those `EmbeddingResponseData` objects contains a single vector embedding inside the `.embedding` attribute. The dimensionality of the embedding model is `1024` which we can see below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiHpHU-9P2zj",
        "outputId": "5aa276e8-0826-4174-cdc0-0aecc2dc577b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(embeddings_response.data[0].embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP1B_-wNQH2l"
      },
      "source": [
        "Next we must add all of these vectors to a single numpy array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NitxTBYxQO5u",
        "outputId": "8c02327e-90ca-461f-f1ad-5b1c0d2681bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(35, 1024)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "video_emb = np.asarray([x.embedding for x in embeddings_response.data])\n",
        "# check the dimensionality of our video chunks array\n",
        "video_emb.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0io3Hze3QiC_"
      },
      "source": [
        "From this we can see we have 35 1024-dimensional vector embeddings that represent our full transcribed document. Now to plug this back into our agent we will create a `query` tool that will allow our LLM to provide a natural language query and return the most relevant chunks based on that query.\n",
        "\n",
        "Let's start by writing this tool step-by-step. First, we create a _query vector_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtpcuqWXSOhJ",
        "outputId": "86088266-f41f-4465-8c54-ad097727468c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1024,)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"what is the relationship between AI agents and GOFAI?\"\n",
        "\n",
        "embeddings_response = await client.embeddings.create_async(\n",
        "    model=\"mistral-embed\",\n",
        "    inputs=[query]\n",
        ")\n",
        "xq = np.asarray(embeddings_response.data[0].embedding)\n",
        "xq.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPV_y0q7SoaG"
      },
      "source": [
        "Now calculate the dot product similarity between our query vector `xq` and precomputed document chunk vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUcs6hOrSvrf",
        "outputId": "11c26e9c-0dc5-4219-e77f-9fc765da6ba7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.6828565 , 0.75058198, 0.69919688, 0.67968833, 0.61341964,\n",
              "       0.60867901, 0.62188296, 0.60628245, 0.60307181, 0.74770941,\n",
              "       0.76357249, 0.61461822, 0.75837007, 0.75699921, 0.64369794,\n",
              "       0.71479521, 0.65117063, 0.68480309, 0.71202544, 0.69951878,\n",
              "       0.68277037, 0.68066402, 0.74412483, 0.64809726, 0.66284248,\n",
              "       0.64783145, 0.62315144, 0.62955026, 0.66573198, 0.70150278,\n",
              "       0.69305011, 0.67002756, 0.69779223, 0.70010112, 0.58219499])"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sim_arr = np.dot(xq, video_emb.T)\n",
        "sim_arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCUmqigGTY5v"
      },
      "source": [
        "Now we return the indexes for the `top_k` most similar (highest scoring) chunks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo73Gxy9T2rq",
        "outputId": "88e38a41-ff9b-435b-dd67-1474505644c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([10, 12, 13])"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "top_k = 3  # we'll set top_k to 3, returning the 3 most similar chunks\n",
        "\n",
        "most_similar_idx = np.argsort(sim_arr)[-top_k:][::-1]\n",
        "most_similar_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFQ8dpm8UI3M"
      },
      "source": [
        "Before pulling the content of each chunk, we convert our list of chunks into an array of chunks â€” these will speed up our chunk content retrieval later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lhJaZT9UPgR",
        "outputId": "ab652f55-993f-42d5-a625-ead19354c380"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(35,)"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks_content = np.asarray([x.content for x in chunks.document.chunks])\n",
        "chunks_content.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gPo6gROUvrl"
      },
      "source": [
        "Now retrieve the chunks content:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_i3SOsRUyMk",
        "outputId": "27641c77-dfc6-4841-a332-5d4954027776"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([\". .  basically agent, LM agent. I think this came just before the React agent paper. It's very similar, I would say, has a bit less structured in the React agent. But, yeah, it's super relevant. And the way that they described their system was that it was a neurosymbolic architecture. I really like this definition because a, so neurosymbolic architecture, It's two things, right? You have the neural part, you have the symbolic part. And I actually have another kind of starting  on this article but it's uh yeah there's this mostly notes at the moment so the neural part of this in fact let's start with the symbolic part the symbolic part is the more traditional AI right so the you know I think this is back in the 40s 50s 60s mostly and then maybe so actually 70s as well this was actually maybe not 70s this was this was a the sort of traditional approach to AI. And the idea,  or the symbolists that were just like full on symbolists felt that true AGI would be achieved through written rules, ontologies and these other logical functions to basically a load of handwritten stuff, like smart, like philosophical grammars. An example of this is the, I think it's syllogistic logic from Aristotle. and the, so basically, an example of this would be a, I think it's got,  you have this major premise then you have a minor premise and i haven't done this for a long time so forgive me if i'm not super accurate um but you have a major premise minor premise and conclusion based on that so the idea is like if you say something like um all all dogs have four legs which is maybe not actually true but let's just assume that you like all dogs have four legs by nature.\",\n",
              "       \"Okay, so this is a logical framework developed by Arisothel. And the symbolic AI people would, you know, do things like these, these sort of exercises where they're going through all this and trying to build up some sort of logical methodology to allow you to kind of construct some deeper, like aGI type system where it can just kind of figure everything out. Now, that was like one side of AI back then. And this is like the traditional AI, it's also called like good,  old fashioned AI.\",\n",
              "       \"I don't remember who or when that was turned, but gofi. I don't know if they actually call it gofi, but that's how it's written. And yeah, I mean, that was one camp. The other camp were the connectionists. So this is what we, we call them back then. Now it's kind of the neural AI type thing. So connectionism was in, so kind of emerged back in 9. there was this basically a paper that described a neural circuit. But really, the where neural or connectionist AI really started with is with this guy, Rosenblatt, who introduced this idea of a perceptron. And it's actually the perceptron is in an adapted version of the perceptron that he described is what we use in neural networks today.\"],\n",
              "      dtype='<U1670')"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks_content[most_similar_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOVPIPzVVENM"
      },
      "source": [
        "We can see here that we've returned the chunks of our article _most_ relevant to our query. Now we wrap all of this up into a single function, ie our _tool_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "g__6FpSuO-QO"
      },
      "outputs": [],
      "source": [
        "async def search(query: str) -> str:\n",
        "    \"\"\"Use this tool to search for relevant chunks of information\n",
        "    from the provided video. Provide as much context as possible\n",
        "    to the `query` parameter, ensuring to write your search\n",
        "    query in natural language. If you must answer multiple\n",
        "    questions you should use this tool to only answer one at a\n",
        "    time. Do not include multiple questions in the `query`.\"\"\"\n",
        "    # embed our query to create a 'query vector'\n",
        "    embeddings_response = await client.embeddings.create_async(\n",
        "        model=\"mistral-embed\",\n",
        "        inputs=[query]\n",
        "    )\n",
        "    xq = np.asarray(embeddings_response.data[0].embedding)\n",
        "    # perform the similarity search\n",
        "    sim_arr = np.dot(xq, video_emb.T)\n",
        "    # get the top_k most similar chunks\n",
        "    most_similar_idx = np.argsort(sim_arr)[-top_k:][::-1]\n",
        "    # return our most relevant chunks\n",
        "    return \"\\n---\\n\".join(chunks_content[most_similar_idx].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE9SB1NzV_D1"
      },
      "source": [
        "Let's test quickly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsJw3Z8tV-b7",
        "outputId": "01360a71-767d-453a-c8ba-6b62e7dd2eea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ". .  basically agent, LM agent. I think this came just before the React agent paper. It's very similar, I would say, has a bit less structured in the React agent. But, yeah, it's super relevant. And the way that they described their system was that it was a neurosymbolic architecture. I really like this definition because a, so neurosymbolic architecture, It's two things, right? You have the neural part, you have the symbolic part. And I actually have another kind of starting  on this article but it's uh yeah there's this mostly notes at the moment so the neural part of this in fact let's start with the symbolic part the symbolic part is the more traditional AI right so the you know I think this is back in the 40s 50s 60s mostly and then maybe so actually 70s as well this was actually maybe not 70s this was this was a the sort of traditional approach to AI. And the idea,  or the symbolists that were just like full on symbolists felt that true AGI would be achieved through written rules, ontologies and these other logical functions to basically a load of handwritten stuff, like smart, like philosophical grammars. An example of this is the, I think it's syllogistic logic from Aristotle. and the, so basically, an example of this would be a, I think it's got,  you have this major premise then you have a minor premise and i haven't done this for a long time so forgive me if i'm not super accurate um but you have a major premise minor premise and conclusion based on that so the idea is like if you say something like um all all dogs have four legs which is maybe not actually true but let's just assume that you like all dogs have four legs by nature.\n",
            "---\n",
            "Okay, so this is a logical framework developed by Arisothel. And the symbolic AI people would, you know, do things like these, these sort of exercises where they're going through all this and trying to build up some sort of logical methodology to allow you to kind of construct some deeper, like aGI type system where it can just kind of figure everything out. Now, that was like one side of AI back then. And this is like the traditional AI, it's also called like good,  old fashioned AI.\n",
            "---\n",
            "I don't remember who or when that was turned, but gofi. I don't know if they actually call it gofi, but that's how it's written. And yeah, I mean, that was one camp. The other camp were the connectionists. So this is what we, we call them back then. Now it's kind of the neural AI type thing. So connectionism was in, so kind of emerged back in 9. there was this basically a paper that described a neural circuit. But really, the where neural or connectionist AI really started with is with this guy, Rosenblatt, who introduced this idea of a perceptron. And it's actually the perceptron is in an adapted version of the perceptron that he described is what we use in neural networks today.\n"
          ]
        }
      ],
      "source": [
        "print(await search(query=query))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJCKrNc3WUH9"
      },
      "source": [
        "Now we need to redefine our `Agent` and plug our new tool into it. To do that we need to format our tool so that the Mistral API can read it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "wo1la0X9X96g",
        "outputId": "602b2c2b-30d7-450c-f4d1-a568575b7ae2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Use this tool to search for relevant chunks of information\\nfrom the provided video. Provide as much context as possible\\nto the `query` parameter, ensuring to write your search\\nquery in natural language. If you must answer multiple \\nquestions you should use this tool to only answer one at a\\ntime. Do not include multiple questions in the `query`.'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import inspect\n",
        "\n",
        "inspect.getdoc(search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Pz6VLVdtIDcg",
        "outputId": "90b3e6dc-51fc-47d9-e992-57efba4832cf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'search'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "search.__name__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_4RAxhmXHVQ",
        "outputId": "9a575518-7b80-4112-f0dd-2d9b2a1b588e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'name': 'search',\n",
              " 'description': 'Use this tool to search for relevant chunks of information\\nfrom the provided video. Provide as much context as possible\\nto the `query` parameter, ensuring to write your search\\nquery in natural language. If you must answer multiple \\nquestions you should use this tool to only answer one at a\\ntime. Do not include multiple questions in the `query`.',\n",
              " 'parameters': {'type': 'object', 'properties': {}, 'required': []}}"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "func_schema = {\n",
        "    \"name\": search.__name__,\n",
        "    \"description\": inspect.getdoc(search),\n",
        "    \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}\n",
        "}\n",
        "func_schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Fg5WaBBBa2hq"
      },
      "outputs": [],
      "source": [
        "dtype_map = {\n",
        "    int: \"number\",\n",
        "    float: \"number\",\n",
        "    str: \"string\",\n",
        "    bool: \"boolean\",\n",
        "    None: \"null\",\n",
        "    list: \"array\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "PANBF-fZY5kf"
      },
      "outputs": [],
      "source": [
        "signature = inspect.signature(search)\n",
        "for name, dtype in signature.parameters.items():\n",
        "    # add param to properties\n",
        "    func_schema[\"parameters\"][\"properties\"][name] = {\n",
        "        \"type\": dtype_map.get(dtype.annotation, \"object\")\n",
        "    }\n",
        "    # and required (assuming all are required)\n",
        "    func_schema[\"parameters\"][\"required\"].append(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDbaaWLrcLP2"
      },
      "source": [
        "We now have our fully defined function schema:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8twTQVHYWC6",
        "outputId": "8b481720-ca6d-41db-f991-cfa31cf0ef54"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'name': 'search',\n",
              " 'description': 'Use this tool to search for relevant chunks of information\\nfrom the provided video. Provide as much context as possible\\nto the `query` parameter, ensuring to write your search\\nquery in natural language. If you must answer multiple \\nquestions you should use this tool to only answer one at a\\ntime. Do not include multiple questions in the `query`.',\n",
              " 'parameters': {'type': 'object',\n",
              "  'properties': {'query': {'type': 'string'}},\n",
              "  'required': ['query']}}"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "func_schema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g24ZBGWCcdmB"
      },
      "source": [
        "We transform this into a `mistralai` `Function` object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "iN7FgO4XcixE"
      },
      "outputs": [],
      "source": [
        "from mistralai.models.function import Function\n",
        "\n",
        "tool_signatures = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": Function(\n",
        "            name=func_schema[\"name\"],\n",
        "            description=func_schema[\"description\"],\n",
        "            parameters=func_schema[\"parameters\"]\n",
        "        )\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg8KM0G6dIcA"
      },
      "source": [
        "We can add our new `tool_signatures` list to our completion call within our `Agent`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "-b3LG6tUWFI2"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    messages: list[AssistantMessage | SystemMessage | UserMessage]\n",
        "    usage: list[UsageInfo]\n",
        "    tool_signatures: list[Function]\n",
        "\n",
        "    def __init__(self, tool_signatures: list[Function]):\n",
        "        self.messages = [\n",
        "            SystemMessage(content=(\n",
        "                \"You are an AI expert providing help to the user \"\n",
        "                \"based on the content of the provided transcribed \"\n",
        "                \"document.\"\n",
        "            ))\n",
        "        ]\n",
        "        self.usage = []\n",
        "        self.tool_signatures = tool_signatures\n",
        "\n",
        "    async def chat(self, content: str) -> AssistantMessage:\n",
        "        # append user message to self.messages\n",
        "        self.messages.append(UserMessage(content=content))\n",
        "        # generate response asynchronously\n",
        "        response = await client.chat.stream_async(\n",
        "            model=\"mistral-large-latest\",\n",
        "            messages=self.messages,\n",
        "            tools=self.tool_signatures,\n",
        "            tool_choice=\"auto\"\n",
        "        )\n",
        "        # full response object to be built\n",
        "        all_tokens = []\n",
        "        all_usage = []\n",
        "        # iterate through the token generator\n",
        "        async for chunk in response:\n",
        "            if isinstance((tool_call := chunk.data.choices[0].delta.tool_calls), list):\n",
        "                print(tool_call)\n",
        "            elif (token := chunk.data.choices[0].delta.content) is not None:\n",
        "                print(token, end=\"\", flush=True)\n",
        "                all_tokens.append(token)\n",
        "        # append assistant message to self.messages\n",
        "        #self.messages.append(AssistantMessage(content=\"\".join(all_tokens)))\n",
        "        # append usage (we can use this later)\n",
        "        #self.usage.append(chunk.data.usage)\n",
        "        return self.messages[-1], tool_call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WijMyLnde7m",
        "outputId": "f884b45e-eb45-43d7-9711-0557df0e94d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ToolCall(function=FunctionCall(name='search', arguments='{\"query\": \"what is the meaning of \\'symbolic\\' in this article?\"}'), id='wnRfREnw4', type=None, index=0)]\n"
          ]
        }
      ],
      "source": [
        "agent = Agent(tool_signatures=tool_signatures)\n",
        "\n",
        "res = await agent.chat(\n",
        "    content=\"can you summarize the meaning of 'symbolic' in this article?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prFazOlqcuHZ"
      },
      "source": [
        "Our video agent can now create a tool call but it cannot execute the tool call â€” for that we need a little more scaffolding to handle the detection of a tool call coming from our LLM and the translation of that into execution of our `search` function.\n",
        "\n",
        "To do that, we will create a tool execution function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "8-UW8uoIdnAv"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from mistralai.models import ToolCall, ToolMessage\n",
        "\n",
        "tools = [search]\n",
        "\n",
        "tool_map = {t.__name__: t for t in tools}\n",
        "\n",
        "async def execute_tool(tool_call: ToolCall) -> ToolMessage:\n",
        "    tool_name = tool_call.function.name\n",
        "    tool_params = json.loads(tool_call.function.arguments)\n",
        "    tool_call_id = tool_call.id\n",
        "    out = await tool_map[tool_name](**tool_params)\n",
        "    return ToolMessage(\n",
        "        content=out,\n",
        "        name=tool_name,\n",
        "        tool_call_id=tool_call_id\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mannii8wlVnA"
      },
      "source": [
        "Now let's take the `tool_call` from our previous `Agent.chat` call and run it through our `execute_tool` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xsM-ZcPkouk",
        "outputId": "77966d40-0e9f-466e-c854-ef8dc26dd6eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ToolCall(function=FunctionCall(name='search', arguments='{\"query\": \"what is the meaning of \\'symbolic\\' in this article?\"}'), id='wnRfREnw4', type=None, index=0)"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res[1][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqHSnf_hlfh7",
        "outputId": "0be27cf8-cb74-44ff-abd9-685a28f9df77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ToolMessage(content=\". .  basically agent, LM agent. I think this came just before the React agent paper. It's very similar, I would say, has a bit less structured in the React agent. But, yeah, it's super relevant. And the way that they described their system was that it was a neurosymbolic architecture. I really like this definition because a, so neurosymbolic architecture, It's two things, right? You have the neural part, you have the symbolic part. And I actually have another kind of starting  on this article but it's uh yeah there's this mostly notes at the moment so the neural part of this in fact let's start with the symbolic part the symbolic part is the more traditional AI right so the you know I think this is back in the 40s 50s 60s mostly and then maybe so actually 70s as well this was actually maybe not 70s this was this was a the sort of traditional approach to AI. And the idea,  or the symbolists that were just like full on symbolists felt that true AGI would be achieved through written rules, ontologies and these other logical functions to basically a load of handwritten stuff, like smart, like philosophical grammars. An example of this is the, I think it's syllogistic logic from Aristotle. and the, so basically, an example of this would be a, I think it's got,  you have this major premise then you have a minor premise and i haven't done this for a long time so forgive me if i'm not super accurate um but you have a major premise minor premise and conclusion based on that so the idea is like if you say something like um all all dogs have four legs which is maybe not actually true but let's just assume that you like all dogs have four legs by nature.\\n---\\nWell, kind of. To some degree, they are almost kind of mixed together already with neural networks, because neural networks, the way that they work, they almost learn symbols, like they learn logical representations of different concepts, which is what the symbol,  part is in some symbolic they learn these right but they're just not handwritten okay so you know neural network kind of learns what are what a strawberry is or what a dog is but anyway it's kind of side the point um we can just assume okay maybe maybe neural networks are subs symbolic but for now let's just assume they're purely symbolic that's fine so neural networks make up the neural part of this so basically LLMs, then we have this symbolic part. The symbolic part, as I mentioned before, it's handwritten stuff, right?\\n---\\nSo. . .  like code. So if you write some like some code that can be run by a or triggered by an LLM or some other type of neural network that you have some sort of neural's neuro symbolic architecture. You have a mix of both.\", tool_call_id='wnRfREnw4', name='search', role='tool')"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tool_message = await execute_tool(tool_call=res[1][0])\n",
        "tool_message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FTnKnowmW_h"
      },
      "source": [
        "This is our executed tool output. We'd append this alongside an `AssistantMessage` for the initial LLM-generated tool call to our `Agent.messages` attribute, then feed everything back into our LLM for it to decide what to do next. Hopefully, we'll see our LLM deciding to use the information it gathered to respond to the user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_HRkSlhloCV",
        "outputId": "f72781f4-2b3a-435d-cf85-fab886b6345e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are an AI expert providing help to the user based on the content of the provided transcribed document.', role='system'),\n",
              " UserMessage(content=\"can you summarize the meaning of 'symbolic' in this article?\", role='user'),\n",
              " AssistantMessage(content='', tool_calls=[ToolCall(function=FunctionCall(name='search', arguments='{\"query\": \"what is the meaning of \\'symbolic\\' in this article?\"}'), id='wnRfREnw4', type=None, index=0)], prefix=False, role='assistant'),\n",
              " ToolMessage(content=\". .  basically agent, LM agent. I think this came just before the React agent paper. It's very similar, I would say, has a bit less structured in the React agent. But, yeah, it's super relevant. And the way that they described their system was that it was a neurosymbolic architecture. I really like this definition because a, so neurosymbolic architecture, It's two things, right? You have the neural part, you have the symbolic part. And I actually have another kind of starting  on this article but it's uh yeah there's this mostly notes at the moment so the neural part of this in fact let's start with the symbolic part the symbolic part is the more traditional AI right so the you know I think this is back in the 40s 50s 60s mostly and then maybe so actually 70s as well this was actually maybe not 70s this was this was a the sort of traditional approach to AI. And the idea,  or the symbolists that were just like full on symbolists felt that true AGI would be achieved through written rules, ontologies and these other logical functions to basically a load of handwritten stuff, like smart, like philosophical grammars. An example of this is the, I think it's syllogistic logic from Aristotle. and the, so basically, an example of this would be a, I think it's got,  you have this major premise then you have a minor premise and i haven't done this for a long time so forgive me if i'm not super accurate um but you have a major premise minor premise and conclusion based on that so the idea is like if you say something like um all all dogs have four legs which is maybe not actually true but let's just assume that you like all dogs have four legs by nature.\\n---\\nWell, kind of. To some degree, they are almost kind of mixed together already with neural networks, because neural networks, the way that they work, they almost learn symbols, like they learn logical representations of different concepts, which is what the symbol,  part is in some symbolic they learn these right but they're just not handwritten okay so you know neural network kind of learns what are what a strawberry is or what a dog is but anyway it's kind of side the point um we can just assume okay maybe maybe neural networks are subs symbolic but for now let's just assume they're purely symbolic that's fine so neural networks make up the neural part of this so basically LLMs, then we have this symbolic part. The symbolic part, as I mentioned before, it's handwritten stuff, right?\\n---\\nSo. . .  like code. So if you write some like some code that can be run by a or triggered by an LLM or some other type of neural network that you have some sort of neural's neuro symbolic architecture. You have a mix of both.\", tool_call_id='wnRfREnw4', name='search', role='tool')]"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent.messages.extend([\n",
        "    AssistantMessage(content=\"\", tool_calls=res[1]),\n",
        "    tool_message\n",
        "])\n",
        "agent.messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfZzI36Bov6z",
        "outputId": "9843f8a8-b68c-451b-e7fc-ad0c5b973a26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In the article, the term \"symbolic\" refers to a traditional approach to artificial intelligence (AI) that relies on written rules, ontologies, and logical functions to achieve true AGI (Artificial General Intelligence). This method involves creating handwritten philosophical grammars and logical representations, such as syllogistic logic from Aristotle, which includes major premises, minor premises, and conclusions. For example, a symbolic approach might state that all dogs have four legs, using this logical structure to define and reason about concepts.\n",
            "\n",
            "The article also discusses \"neurosymbolic architecture,\" which combines both symbolic and neural components. The symbolic part involves handwritten code or rules that can be triggered by a large language model (LLM) or another type of neural network. Neural networks, on the other hand, learn logical representations of different concepts, such as what a strawberry or a dog is, without relying on handwritten rules.\n",
            "\n",
            "In summary, \"symbolic\" in this context refers to the use of predefined rules and logical structures to represent and reason about knowledge in AI systems."
          ]
        }
      ],
      "source": [
        "# generate response asynchronously\n",
        "response = await client.chat.stream_async(\n",
        "    model=\"mistral-large-latest\",\n",
        "    messages=agent.messages,\n",
        "    tools=agent.tool_signatures,\n",
        "    tool_choice=\"auto\"\n",
        ")\n",
        "# full response object to be built\n",
        "all_tokens = []\n",
        "all_usage = []\n",
        "# iterate through the token generator and add to queue\n",
        "async for chunk in response:\n",
        "    if isinstance((tool_call := chunk.data.choices[0].delta.tool_calls), list):\n",
        "        print(tool_call)\n",
        "    elif (token := chunk.data.choices[0].delta.content) is not None:\n",
        "        print(token, end=\"\", flush=True)\n",
        "        all_tokens.append(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDWV8-UvpWSA"
      },
      "source": [
        "After adding these additional tool call messages our LLM is able to respond to our query directly. Now let's integrate all of this back into a new `Agent` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "8Q1LKi6WpU5F"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    messages: list[AssistantMessage | SystemMessage | UserMessage]\n",
        "    usage: list[UsageInfo]\n",
        "    tool_signatures: list[Function]\n",
        "\n",
        "    def __init__(self, tool_signatures: list[Function], max_steps: int = 3):\n",
        "        self.messages = [\n",
        "            SystemMessage(content=(\n",
        "                \"You are an AI expert providing help to the user \"\n",
        "                \"based on the content of the provided transcribed \"\n",
        "                \"document.\"\n",
        "            ))\n",
        "        ]\n",
        "        self.usage = []\n",
        "        self.tool_signatures = tool_signatures\n",
        "        self.max_steps = max_steps\n",
        "\n",
        "    async def chat(self, content: str) -> AssistantMessage:\n",
        "        # append user message to self.messages\n",
        "        self.messages.append(UserMessage(content=content))\n",
        "        # we will need to enter a loop to support multiple iterations\n",
        "        step = 0\n",
        "        while step <= self.max_steps:\n",
        "            # generate response asynchronously\n",
        "            response = await client.chat.stream_async(\n",
        "                model=\"mistral-large-latest\",\n",
        "                messages=self.messages,\n",
        "                tools=self.tool_signatures,\n",
        "                tool_choice=\"auto\"\n",
        "            )\n",
        "            # full response object to be built\n",
        "            all_tokens = []\n",
        "            all_usage = []\n",
        "            # iterate through the token generator and add to queue\n",
        "            async for chunk in response:\n",
        "                if isinstance(\n",
        "                    (tool_calls := chunk.data.choices[0].delta.tool_calls), list\n",
        "                ):\n",
        "                    # print the tool call in a cleaner format\n",
        "                    print(\n",
        "                        f\"{tool_calls[0].function.name}: \"\n",
        "                        f\"{tool_calls[0].function.arguments}\"\n",
        "                    )\n",
        "                    # we execute our tool\n",
        "                    tool_message = await execute_tool(tool_call=tool_calls[0])\n",
        "                    # and add the assistant tool call and tool output message\n",
        "                    # to our self.messages\n",
        "                    self.messages.extend([\n",
        "                        AssistantMessage(content=\"\", tool_calls=tool_calls),\n",
        "                        tool_message\n",
        "                    ])\n",
        "\n",
        "                elif (token := chunk.data.choices[0].delta.content) is not None:\n",
        "                    print(token, end=\"\", flush=True)\n",
        "                    all_tokens.append(token)\n",
        "            # append usage (we can use this later)\n",
        "            self.usage.append(chunk.data.usage)\n",
        "            # append assistant message to self.messages (if returned)\n",
        "            if len(all_tokens) > 1:\n",
        "                self.messages.append(AssistantMessage(content=\"\".join(all_tokens)))\n",
        "                break\n",
        "            step += 1\n",
        "        return self.messages[-1], tool_call"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_QkMXZkDVnq"
      },
      "source": [
        "Now let's try another query, this time we will try to allow our agent to use the `search` tool twice to collate information from various chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoO49sb1Ajen",
        "outputId": "87b36f11-50be-4e05-e62b-2f32c8044595"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "search: {\"query\": \"good old fashioned AI\"}\n",
            "search: {\"query\": \"deepseek\"}\n",
            "Yes, the document mentions 'good old fashioned AI'. This term is used to refer to traditional AI approaches, often called symbolic AI. The document describes this type of AI as involving logical frameworks and methodologies, such as those developed by Aristotle. It involves constructing deeper AI systems that can figure things out, using exercises and logical frameworks.\n",
            "\n",
            "The document does not explicitly mention 'deepseek'.\n",
            "\n",
            "The document compares what could be inferred as more modern AI approaches, involving neural networks and connectionist AI, with the traditional 'good old fashioned AI'. The traditional AI is described as involving written rules, ontologies, and logical functions, aiming to achieve AGI through handwritten philosophical grammars and logical frameworks like syllogistic logic. In contrast, the more modern approaches, which could be compared to 'deepseek' (though not explicitly mentioned), involve neural networks and have seen resurgences in interest, particularly after developments like the AlexNet model and ImageNet. These modern approaches are described as amazing and have sparked significant interest in the field."
          ]
        }
      ],
      "source": [
        "agent = Agent(tool_signatures=tool_signatures)\n",
        "\n",
        "res = await agent.chat(\n",
        "    content=(\n",
        "        \"Does the document mention 'good old fashioned AI'? And does it \"\n",
        "        \"say anything about deepseek? How does the document \"\n",
        "        \"compare the two?\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2T_Fg1GGj0H"
      },
      "source": [
        "Despite performing two searches to answer this query, we still used _significantly_ less tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqj1KDlICErf",
        "outputId": "9b028479-8d57-4fff-e9b7-4eb8031c7bd3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[UsageInfo(prompt_tokens=187, completion_tokens=23, total_tokens=210),\n",
              " UsageInfo(prompt_tokens=898, completion_tokens=20, total_tokens=918),\n",
              " UsageInfo(prompt_tokens=1727, completion_tokens=228, total_tokens=1955)]"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent.usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8XIE_6sGsX3"
      },
      "source": [
        "Without chunking we spent ~$0.12 on a single query (with a single question):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dthl3iGwMnyR",
        "outputId": "c8f11b5b-4349-4bd4-c813-740dc38b7d2b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.0125, 0.01475, 0.0157]"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "original_cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0izRY9oXM4Dq"
      },
      "source": [
        "With chunking, for two questions within a single query we're spending:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuEbgTMIGPCs",
        "outputId": "4968d7a5-aa07-4cbc-af4e-9fba17e7d267"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "$0.00051\n",
            "$0.00192\n",
            "$0.00482\n"
          ]
        }
      ],
      "source": [
        "for usage in agent.usage:\n",
        "    print(f\"${cost(usage=usage)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJFyhuRZMZUA"
      },
      "source": [
        "A total of ~$0.0072, almost a 50% reduction in price despite being a more complex query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HODV8EEHNKeD"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dthGXq8rMDTD"
      },
      "source": [
        "With that we've built an agent capable of helping us understand videos. We've also taken steps to drastically optimize expenditure."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
